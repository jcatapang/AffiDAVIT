{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic dependencies \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from PIL.Image import open as pil_open\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "\n",
    "# import dependencies for evaluation\n",
    "from transformers import AutoFeatureExtractor, ViTForImageClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Features, Image, Value\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 35126/35126 [00:02<00:00, 15118.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 35126/35126 [00:00<00:00, 1112656.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>path</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_left</td>\n",
       "      <td>data/images/10_left.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_right</td>\n",
       "      <td>data/images/10_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13_left</td>\n",
       "      <td>data/images/13_left.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_right</td>\n",
       "      <td>data/images/13_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15_left</td>\n",
       "      <td>data/images/15_left.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15_right</td>\n",
       "      <td>data/images/15_right.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16_left</td>\n",
       "      <td>data/images/16_left.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16_right</td>\n",
       "      <td>data/images/16_right.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17_left</td>\n",
       "      <td>data/images/17_left.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17_right</td>\n",
       "      <td>data/images/17_right.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image                       path  level\n",
       "0   10_left   data/images/10_left.jpeg      0\n",
       "1  10_right  data/images/10_right.jpeg      0\n",
       "2   13_left   data/images/13_left.jpeg      0\n",
       "3  13_right  data/images/13_right.jpeg      0\n",
       "4   15_left   data/images/15_left.jpeg      1\n",
       "5  15_right  data/images/15_right.jpeg      1\n",
       "6   16_left   data/images/16_left.jpeg      1\n",
       "7  16_right  data/images/16_right.jpeg      1\n",
       "8   17_left   data/images/17_left.jpeg      0\n",
       "9  17_right  data/images/17_right.jpeg      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df = pd.read_csv(\"data/labels.csv\")\n",
    "master_df[\"path\"] = \"data/images/\" + master_df[\"image\"] + \".jpeg\"\n",
    "master_df[\"exist\"] = master_df[\"path\"].progress_apply(os.path.exists)\n",
    "master_df[\"level\"] = master_df[\"level\"].progress_apply(lambda x: x if x == 0 else 1)\n",
    "master_df = master_df[master_df[\"exist\"] == True]\n",
    "master_df = master_df[[\"image\", \"path\", \"level\"]]\n",
    "master_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>path</th>\n",
       "      <th>level</th>\n",
       "      <th>updated_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4593_right</td>\n",
       "      <td>data/images/4593_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>data\\preprocessed\\train\\no_dr\\4593_right.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18542_right</td>\n",
       "      <td>data/images/18542_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>data\\preprocessed\\train\\no_dr\\18542_right.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16973_right</td>\n",
       "      <td>data/images/16973_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>data\\preprocessed\\train\\no_dr\\16973_right.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33928_right</td>\n",
       "      <td>data/images/33928_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>data\\preprocessed\\train\\no_dr\\33928_right.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36713_right</td>\n",
       "      <td>data/images/36713_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>data\\preprocessed\\train\\no_dr\\36713_right.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image                          path  level  \\\n",
       "0   4593_right   data/images/4593_right.jpeg      0   \n",
       "1  18542_right  data/images/18542_right.jpeg      0   \n",
       "2  16973_right  data/images/16973_right.jpeg      0   \n",
       "3  33928_right  data/images/33928_right.jpeg      0   \n",
       "4  36713_right  data/images/36713_right.jpeg      0   \n",
       "\n",
       "                                     updated_path  \n",
       "0   data\\preprocessed\\train\\no_dr\\4593_right.jpeg  \n",
       "1  data\\preprocessed\\train\\no_dr\\18542_right.jpeg  \n",
       "2  data\\preprocessed\\train\\no_dr\\16973_right.jpeg  \n",
       "3  data\\preprocessed\\train\\no_dr\\33928_right.jpeg  \n",
       "4  data\\preprocessed\\train\\no_dr\\36713_right.jpeg  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train_labels.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>path</th>\n",
       "      <th>level</th>\n",
       "      <th>updated_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16076_right</td>\n",
       "      <td>data/images/16076_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>data\\preprocessed\\test\\no_dr\\16076_right.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5680_right</td>\n",
       "      <td>data/images/5680_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>data\\preprocessed\\test\\no_dr\\5680_right.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31209_right</td>\n",
       "      <td>data/images/31209_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>data\\preprocessed\\test\\no_dr\\31209_right.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44088_left</td>\n",
       "      <td>data/images/44088_left.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>data\\preprocessed\\test\\no_dr\\44088_left.jpeg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41035_left</td>\n",
       "      <td>data/images/41035_left.jpeg</td>\n",
       "      <td>0</td>\n",
       "      <td>data\\preprocessed\\test\\no_dr\\41035_left.jpeg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         image                          path  level  \\\n",
       "0  16076_right  data/images/16076_right.jpeg      0   \n",
       "1   5680_right   data/images/5680_right.jpeg      0   \n",
       "2  31209_right  data/images/31209_right.jpeg      0   \n",
       "3   44088_left   data/images/44088_left.jpeg      0   \n",
       "4   41035_left   data/images/41035_left.jpeg      0   \n",
       "\n",
       "                                    updated_path  \n",
       "0  data\\preprocessed\\test\\no_dr\\16076_right.jpeg  \n",
       "1   data\\preprocessed\\test\\no_dr\\5680_right.jpeg  \n",
       "2  data\\preprocessed\\test\\no_dr\\31209_right.jpeg  \n",
       "3   data\\preprocessed\\test\\no_dr\\44088_left.jpeg  \n",
       "4   data\\preprocessed\\test\\no_dr\\41035_left.jpeg  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"data/test_labels.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_images = list(train_df[\"image\"].values.tolist())\n",
    "test_df_images = list(test_df[\"image\"].values.tolist())\n",
    "train_test_images = train_df_images + test_df_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    558\n",
       "1    558\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[\"level\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    21158\n",
      "1     7636\n",
      "Name: level, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>path</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_left</td>\n",
       "      <td>data/images/10_left.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_right</td>\n",
       "      <td>data/images/10_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13_left</td>\n",
       "      <td>data/images/13_left.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_right</td>\n",
       "      <td>data/images/13_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15_right</td>\n",
       "      <td>data/images/15_right.jpeg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image                       path  level\n",
       "0   10_left   data/images/10_left.jpeg      0\n",
       "1  10_right  data/images/10_right.jpeg      0\n",
       "2   13_left   data/images/13_left.jpeg      0\n",
       "3  13_right  data/images/13_right.jpeg      0\n",
       "5  15_right  data/images/15_right.jpeg      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df = master_df[~master_df[\"image\"].isin(train_test_images)]\n",
    "print(master_df[\"level\"].value_counts())\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the desired number of samples for each level\n",
    "level_counts = master_df[\"level\"].value_counts()\n",
    "desired_samples = 560\n",
    "\n",
    "# Initialize an empty list to store the selected samples\n",
    "selected_samples = []\n",
    "\n",
    "# Select the desired number of samples for each level\n",
    "for level in level_counts.index:\n",
    "    level_df = master_df[master_df[\"level\"] == level]\n",
    "    selected_samples.append(level_df.sample(desired_samples, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    560\n",
      "1    560\n",
      "Name: level, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>path</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32186</th>\n",
       "      <td>40650_left</td>\n",
       "      <td>data/images/40650_left.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29214</th>\n",
       "      <td>36986_left</td>\n",
       "      <td>data/images/36986_left.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9694</th>\n",
       "      <td>12210_left</td>\n",
       "      <td>data/images/12210_left.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15184</th>\n",
       "      <td>19014_left</td>\n",
       "      <td>data/images/19014_left.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32411</th>\n",
       "      <td>40913_right</td>\n",
       "      <td>data/images/40913_right.jpeg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             image                          path  level\n",
       "32186   40650_left   data/images/40650_left.jpeg      0\n",
       "29214   36986_left   data/images/36986_left.jpeg      0\n",
       "9694    12210_left   data/images/12210_left.jpeg      0\n",
       "15184   19014_left   data/images/19014_left.jpeg      0\n",
       "32411  40913_right  data/images/40913_right.jpeg      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df = pd.concat(selected_samples)\n",
    "print(eval_df[\"level\"].value_counts())\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_dict = {0: \"no_dr\", 1: \"with_dr\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1120/1120 [00:18<00:00, 59.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the rows of the dataframe\n",
    "eval_new_path = list()\n",
    "for index, row in tqdm(eval_df.iterrows(), total=eval_df.shape[0]):\n",
    "    level = row[\"level\"]\n",
    "    file_path = row[\"path\"]\n",
    "    \n",
    "    # Create a folder for the level if it doesn't exist\n",
    "    level_folder = \"data\\\\preprocessed\\\\eval\\\\\" + str(dr_dict[level])\n",
    "    if not os.path.exists(level_folder):\n",
    "        os.makedirs(level_folder)\n",
    "    \n",
    "    # Save the file to the level folder\n",
    "    destination_path = os.path.join(level_folder, os.path.basename(file_path))\n",
    "    shutil.copy(file_path, destination_path)\n",
    "    \n",
    "    # Open the image\n",
    "    image = pil_open(destination_path)\n",
    "    \n",
    "    # Crop the image to hxh, centered on the middle of the image\n",
    "    w, h = image.size\n",
    "    left = (w - h) / 2\n",
    "    top = 0\n",
    "    right = (w + h) / 2\n",
    "    bottom = h\n",
    "    image = image.crop((left, top, right, bottom))\n",
    "    \n",
    "    # Resize the image from hxh to 512x512\n",
    "    new_dim = 512\n",
    "    image = image.resize((new_dim, new_dim))\n",
    "    \n",
    "    # Save the cropped image back to the destination path\n",
    "    image.save(destination_path)\n",
    "    eval_new_path.append(destination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df[\"updated_path\"] = eval_new_path\n",
    "# eval_df.to_csv(\"data/preprocessed/eval_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Features(\n",
    "    {\n",
    "        \"label\": ClassLabel(\n",
    "            num_classes=2,\n",
    "            names=[\"no_dr\", \"with_dr\"],\n",
    "        ),\n",
    "        \"image\": Image()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c64db0e4354fc687b34b4a63601418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration preprocessed-fe7bea385f0ce62c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/preprocessed to C:/Users/Admin/.cache/huggingface/datasets/imagefolder/preprocessed-fe7bea385f0ce62c/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cd201998d249aea524cc4acea084e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ad251fa8d3469fa5d34c2bc3900713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3b13afc8954fb08a633a4dd83fea36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2592c7d82ef447a9ea00c012ed39534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to C:/Users/Admin/.cache/huggingface/datasets/imagefolder/preprocessed-fe7bea385f0ce62c/0.0.0/37fbb85cc714a338bea574ac6c7d0b5be5aff46c1862c1989b20e0771199e93f. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7ef51cf8ce46fc84e671516e9d9905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluation = \"C:\\\\Users\\\\Admin\\\\Documents\\\\Python Scripts\\\\diabetic_retinopathy\\\\data\\\\preprocessed\\\\eval\\\\**\" \n",
    "dataset_eval = load_dataset(path=\"data/preprocessed\", data_files={evaluation}, features=features)\n",
    "labels = dataset_eval[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'image'],\n",
       "        num_rows: 1120\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor([x.convert(\"RGB\").resize((224,224)) for x in example_batch[\"image\"]], return_tensors=\"pt\")\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs[\"labels\"] = example_batch[\"label\"]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_ds_eval = dataset_eval.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
    "        \"labels\": torch.tensor([x[\"labels\"] for x in batch])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return {\n",
    "        \"accuracy\": metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids),\n",
    "        \"confusion_matrix\": confusion_matrix(p.label_ids, np.argmax(p.predictions, axis=1))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vit-dr-base-lr_1e-05-cos-w-res-wd_0.01',\n",
       " 'vit-dr-base-lr_3e-05-cos-w-res-wd_0.01',\n",
       " 'vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01',\n",
       " 'vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = [name for name in os.listdir(\".\") if os.path.isdir(name) and \"vit-dr\" in name]\n",
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vit-dr-base-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>0.821503</td>\n",
       "      <td>0.516895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vit-dr-base-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>0.801081</td>\n",
       "      <td>0.571689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vit-dr-base-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>300</td>\n",
       "      <td>0.792707</td>\n",
       "      <td>0.602740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vit-dr-base-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>400</td>\n",
       "      <td>0.750175</td>\n",
       "      <td>0.636530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vit-dr-base-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>0.775402</td>\n",
       "      <td>0.625571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vit-dr-base-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>600</td>\n",
       "      <td>0.728327</td>\n",
       "      <td>0.648402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vit-dr-base-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>700</td>\n",
       "      <td>0.716609</td>\n",
       "      <td>0.654795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vit-dr-base-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>800</td>\n",
       "      <td>0.739520</td>\n",
       "      <td>0.654795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vit-dr-base-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>900</td>\n",
       "      <td>0.743820</td>\n",
       "      <td>0.649315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vit-dr-base-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.722425</td>\n",
       "      <td>0.657534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    model  step  eval_loss  eval_accuracy\n",
       "0  vit-dr-base-lr_1e-05-cos-w-res-wd_0.01   100   0.821503       0.516895\n",
       "1  vit-dr-base-lr_1e-05-cos-w-res-wd_0.01   200   0.801081       0.571689\n",
       "2  vit-dr-base-lr_1e-05-cos-w-res-wd_0.01   300   0.792707       0.602740\n",
       "3  vit-dr-base-lr_1e-05-cos-w-res-wd_0.01   400   0.750175       0.636530\n",
       "4  vit-dr-base-lr_1e-05-cos-w-res-wd_0.01   500   0.775402       0.625571\n",
       "5  vit-dr-base-lr_1e-05-cos-w-res-wd_0.01   600   0.728327       0.648402\n",
       "6  vit-dr-base-lr_1e-05-cos-w-res-wd_0.01   700   0.716609       0.654795\n",
       "7  vit-dr-base-lr_1e-05-cos-w-res-wd_0.01   800   0.739520       0.654795\n",
       "8  vit-dr-base-lr_1e-05-cos-w-res-wd_0.01   900   0.743820       0.649315\n",
       "9  vit-dr-base-lr_1e-05-cos-w-res-wd_0.01  1000   0.722425       0.657534"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vit-dr-base-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>0.995195</td>\n",
       "      <td>0.520548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vit-dr-base-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>0.796279</td>\n",
       "      <td>0.608219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vit-dr-base-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>300</td>\n",
       "      <td>0.711154</td>\n",
       "      <td>0.633790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vit-dr-base-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>400</td>\n",
       "      <td>0.683450</td>\n",
       "      <td>0.659361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vit-dr-base-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>0.718489</td>\n",
       "      <td>0.657534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vit-dr-base-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>600</td>\n",
       "      <td>0.757318</td>\n",
       "      <td>0.670320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vit-dr-base-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>700</td>\n",
       "      <td>0.713632</td>\n",
       "      <td>0.692237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vit-dr-base-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>800</td>\n",
       "      <td>0.769052</td>\n",
       "      <td>0.688584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vit-dr-base-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>900</td>\n",
       "      <td>0.752040</td>\n",
       "      <td>0.684932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vit-dr-base-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.749811</td>\n",
       "      <td>0.693151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    model  step  eval_loss  eval_accuracy\n",
       "0  vit-dr-base-lr_3e-05-cos-w-res-wd_0.01   100   0.995195       0.520548\n",
       "1  vit-dr-base-lr_3e-05-cos-w-res-wd_0.01   200   0.796279       0.608219\n",
       "2  vit-dr-base-lr_3e-05-cos-w-res-wd_0.01   300   0.711154       0.633790\n",
       "3  vit-dr-base-lr_3e-05-cos-w-res-wd_0.01   400   0.683450       0.659361\n",
       "4  vit-dr-base-lr_3e-05-cos-w-res-wd_0.01   500   0.718489       0.657534\n",
       "5  vit-dr-base-lr_3e-05-cos-w-res-wd_0.01   600   0.757318       0.670320\n",
       "6  vit-dr-base-lr_3e-05-cos-w-res-wd_0.01   700   0.713632       0.692237\n",
       "7  vit-dr-base-lr_3e-05-cos-w-res-wd_0.01   800   0.769052       0.688584\n",
       "8  vit-dr-base-lr_3e-05-cos-w-res-wd_0.01   900   0.752040       0.684932\n",
       "9  vit-dr-base-lr_3e-05-cos-w-res-wd_0.01  1000   0.749811       0.693151"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>1.048261</td>\n",
       "      <td>0.527854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>0.775979</td>\n",
       "      <td>0.595434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>300</td>\n",
       "      <td>0.785580</td>\n",
       "      <td>0.589041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>400</td>\n",
       "      <td>0.766931</td>\n",
       "      <td>0.606393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>0.726594</td>\n",
       "      <td>0.636530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>600</td>\n",
       "      <td>0.719086</td>\n",
       "      <td>0.652968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>700</td>\n",
       "      <td>0.709554</td>\n",
       "      <td>0.652055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>800</td>\n",
       "      <td>0.759712</td>\n",
       "      <td>0.649315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>900</td>\n",
       "      <td>0.741890</td>\n",
       "      <td>0.643836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.727547</td>\n",
       "      <td>0.650228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model  step  eval_loss  eval_accuracy\n",
       "0  vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01   100   1.048261       0.527854\n",
       "1  vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01   200   0.775979       0.595434\n",
       "2  vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01   300   0.785580       0.589041\n",
       "3  vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01   400   0.766931       0.606393\n",
       "4  vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01   500   0.726594       0.636530\n",
       "5  vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01   600   0.719086       0.652968\n",
       "6  vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01   700   0.709554       0.652055\n",
       "7  vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01   800   0.759712       0.649315\n",
       "8  vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01   900   0.741890       0.643836\n",
       "9  vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01  1000   0.727547       0.650228"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>1.099175</td>\n",
       "      <td>0.540639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>200</td>\n",
       "      <td>0.773464</td>\n",
       "      <td>0.597260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>300</td>\n",
       "      <td>0.785473</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>400</td>\n",
       "      <td>0.733104</td>\n",
       "      <td>0.627397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>500</td>\n",
       "      <td>0.687079</td>\n",
       "      <td>0.663014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>600</td>\n",
       "      <td>0.784282</td>\n",
       "      <td>0.647489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>700</td>\n",
       "      <td>0.676528</td>\n",
       "      <td>0.667580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>800</td>\n",
       "      <td>0.712234</td>\n",
       "      <td>0.677626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>900</td>\n",
       "      <td>0.776739</td>\n",
       "      <td>0.670320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.724376</td>\n",
       "      <td>0.678539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  model  step  eval_loss  eval_accuracy\n",
       "0  vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01   100   1.099175       0.540639\n",
       "1  vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01   200   0.773464       0.597260\n",
       "2  vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01   300   0.785473       0.600000\n",
       "3  vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01   400   0.733104       0.627397\n",
       "4  vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01   500   0.687079       0.663014\n",
       "5  vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01   600   0.784282       0.647489\n",
       "6  vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01   700   0.676528       0.667580\n",
       "7  vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01   800   0.712234       0.677626\n",
       "8  vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01   900   0.776739       0.670320\n",
       "9  vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01  1000   0.724376       0.678539"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model_name in model_names:\n",
    "    path = model_name+\"/trainer_state.json\"\n",
    "    epochs = list()\n",
    "\n",
    "    with open(path) as file:\n",
    "        #readlines() reads the file as string and loads() loads it into a dict\n",
    "        obj = json.loads(''.join(file.readlines()))[\"log_history\"]\n",
    "        for epoch in obj:\n",
    "            if \"eval_accuracy\" in epoch:\n",
    "                epochs.append(epoch)\n",
    "    epoch_df = pd.DataFrame(epochs)\n",
    "    epoch_df[\"model\"] = model_name\n",
    "    epoch_df = epoch_df[[\"model\", \"step\", \"eval_loss\", \"eval_accuracy\"]]\n",
    "    display(epoch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1120\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file vit-dr-base-lr_3e-05-cos-w-res-wd_0.01\\preprocessor_config.json\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "loading configuration file vit-dr-base-lr_3e-05-cos-w-res-wd_0.01\\config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no_dr\",\n",
      "    \"1\": \"with_dr\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"no_dr\": \"0\",\n",
      "    \"with_dr\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\"\n",
      "}\n",
      "\n",
      "loading weights file vit-dr-base-lr_3e-05-cos-w-res-wd_0.01\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "All the weights of ViTForImageClassification were initialized from the model checkpoint at vit-dr-base-lr_3e-05-cos-w-res-wd_0.01.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1120\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01\\preprocessor_config.json\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "loading configuration file vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01\\config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no_dr\",\n",
      "    \"1\": \"with_dr\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"no_dr\": \"0\",\n",
      "    \"with_dr\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\"\n",
      "}\n",
      "\n",
      "loading weights file vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "All the weights of ViTForImageClassification were initialized from the model checkpoint at vit-dr-sd-lr_1e-05-cos-w-res-wd_0.01.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1120\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01\\preprocessor_config.json\n",
      "Feature extractor ViTFeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"resample\": 2,\n",
      "  \"size\": 224\n",
      "}\n",
      "\n",
      "loading configuration file vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01\\config.json\n",
      "Model config ViTConfig {\n",
      "  \"_name_or_path\": \"google/vit-base-patch16-224\",\n",
      "  \"architectures\": [\n",
      "    \"ViTForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"no_dr\",\n",
      "    \"1\": \"with_dr\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"no_dr\": \"0\",\n",
      "    \"with_dr\": \"1\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\"\n",
      "}\n",
      "\n",
      "loading weights file vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
      "\n",
      "All the weights of ViTForImageClassification were initialized from the model checkpoint at vit-dr-sd-lr_3e-05-cos-w-res-wd_0.01.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1120\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_metrics = list()\n",
    "for model_name_or_path in model_names:\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=len(labels),\n",
    "        id2label={str(i): c for i, c in enumerate(labels)},\n",
    "        label2id={c: str(i) for i, c in enumerate(labels)},\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"output\",\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        max_steps=1000,\n",
    "        save_steps=100,\n",
    "        eval_steps=100,\n",
    "        save_total_limit=10,\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,\n",
    "        load_best_model_at_end=True,\n",
    "        logging_steps=5,\n",
    "        report_to=\"none\",\n",
    "        lr_scheduler_type=\"cosine_with_restarts\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=prepared_ds_eval[\"train\"],\n",
    "        eval_dataset=prepared_ds_eval[\"train\"],\n",
    "        tokenizer=feature_extractor,\n",
    "    )\n",
    "    \n",
    "    metrics = trainer.evaluate(prepared_ds_eval['train'])\n",
    "    eval_metrics.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(val):\n",
    "    f = \"font-weight: bold\" \n",
    "    #condition\n",
    "    m = val[\"eval_accuracy\"] == 0.7\n",
    "    # DataFrame of styles\n",
    "    df1 = pd.DataFrame('', index=val.index, columns=val.columns)\n",
    "    # set columns by condition\n",
    "    df1 = df1.mask(m, f)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'eval_loss': 0.7349197864532471,\n",
       "  'eval_accuracy': {'accuracy': 0.6553571428571429},\n",
       "  'eval_confusion_matrix': array([[529,  31],\n",
       "         [355, 205]], dtype=int64),\n",
       "  'eval_runtime': 23.9958,\n",
       "  'eval_samples_per_second': 46.675,\n",
       "  'eval_steps_per_second': 1.459},\n",
       " {'eval_loss': 0.6955434679985046,\n",
       "  'eval_accuracy': {'accuracy': 0.6508928571428572},\n",
       "  'eval_confusion_matrix': array([[530,  30],\n",
       "         [361, 199]], dtype=int64),\n",
       "  'eval_runtime': 11.0272,\n",
       "  'eval_samples_per_second': 101.567,\n",
       "  'eval_steps_per_second': 3.174},\n",
       " {'eval_loss': 0.6853053569793701,\n",
       "  'eval_accuracy': {'accuracy': 0.6544642857142857},\n",
       "  'eval_confusion_matrix': array([[521,  39],\n",
       "         [348, 212]], dtype=int64),\n",
       "  'eval_runtime': 11.1602,\n",
       "  'eval_samples_per_second': 100.357,\n",
       "  'eval_steps_per_second': 3.136},\n",
       " {'eval_loss': 0.6213908791542053,\n",
       "  'eval_accuracy': {'accuracy': 0.7},\n",
       "  'eval_confusion_matrix': array([[515,  45],\n",
       "         [291, 269]], dtype=int64),\n",
       "  'eval_runtime': 11.037,\n",
       "  'eval_samples_per_second': 101.477,\n",
       "  'eval_steps_per_second': 3.171}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5915a_row3_col0, #T_5915a_row3_col1, #T_5915a_row3_col2, #T_5915a_row3_col3, #T_5915a_row3_col4, #T_5915a_row3_col5, #T_5915a_row3_col6, #T_5915a_row3_col7, #T_5915a_row3_col8, #T_5915a_row3_col9 {\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5915a_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >base_model_name</th>\n",
       "      <th class=\"col_heading level0 col1\" >finetuned_model_name</th>\n",
       "      <th class=\"col_heading level0 col2\" >finetune_data</th>\n",
       "      <th class=\"col_heading level0 col3\" >learning_rate</th>\n",
       "      <th class=\"col_heading level0 col4\" >eval_loss</th>\n",
       "      <th class=\"col_heading level0 col5\" >eval_accuracy</th>\n",
       "      <th class=\"col_heading level0 col6\" >eval_tn</th>\n",
       "      <th class=\"col_heading level0 col7\" >eval_fp</th>\n",
       "      <th class=\"col_heading level0 col8\" >eval_fn</th>\n",
       "      <th class=\"col_heading level0 col9\" >eval_tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5915a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5915a_row0_col0\" class=\"data row0 col0\" >ViT base patch-16 224</td>\n",
       "      <td id=\"T_5915a_row0_col1\" class=\"data row0 col1\" >ViT-DR</td>\n",
       "      <td id=\"T_5915a_row0_col2\" class=\"data row0 col2\" >base</td>\n",
       "      <td id=\"T_5915a_row0_col3\" class=\"data row0 col3\" >0.000010</td>\n",
       "      <td id=\"T_5915a_row0_col4\" class=\"data row0 col4\" >0.734920</td>\n",
       "      <td id=\"T_5915a_row0_col5\" class=\"data row0 col5\" >0.655357</td>\n",
       "      <td id=\"T_5915a_row0_col6\" class=\"data row0 col6\" >529</td>\n",
       "      <td id=\"T_5915a_row0_col7\" class=\"data row0 col7\" >31</td>\n",
       "      <td id=\"T_5915a_row0_col8\" class=\"data row0 col8\" >355</td>\n",
       "      <td id=\"T_5915a_row0_col9\" class=\"data row0 col9\" >205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5915a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5915a_row1_col0\" class=\"data row1 col0\" >ViT base patch-16 224</td>\n",
       "      <td id=\"T_5915a_row1_col1\" class=\"data row1 col1\" >ViT-DR</td>\n",
       "      <td id=\"T_5915a_row1_col2\" class=\"data row1 col2\" >base</td>\n",
       "      <td id=\"T_5915a_row1_col3\" class=\"data row1 col3\" >0.000030</td>\n",
       "      <td id=\"T_5915a_row1_col4\" class=\"data row1 col4\" >0.695543</td>\n",
       "      <td id=\"T_5915a_row1_col5\" class=\"data row1 col5\" >0.650893</td>\n",
       "      <td id=\"T_5915a_row1_col6\" class=\"data row1 col6\" >530</td>\n",
       "      <td id=\"T_5915a_row1_col7\" class=\"data row1 col7\" >30</td>\n",
       "      <td id=\"T_5915a_row1_col8\" class=\"data row1 col8\" >361</td>\n",
       "      <td id=\"T_5915a_row1_col9\" class=\"data row1 col9\" >199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5915a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5915a_row2_col0\" class=\"data row2 col0\" >ViT base patch-16 224</td>\n",
       "      <td id=\"T_5915a_row2_col1\" class=\"data row2 col1\" >ViT-DR</td>\n",
       "      <td id=\"T_5915a_row2_col2\" class=\"data row2 col2\" >base + diffusion augmentation</td>\n",
       "      <td id=\"T_5915a_row2_col3\" class=\"data row2 col3\" >0.000010</td>\n",
       "      <td id=\"T_5915a_row2_col4\" class=\"data row2 col4\" >0.685305</td>\n",
       "      <td id=\"T_5915a_row2_col5\" class=\"data row2 col5\" >0.654464</td>\n",
       "      <td id=\"T_5915a_row2_col6\" class=\"data row2 col6\" >521</td>\n",
       "      <td id=\"T_5915a_row2_col7\" class=\"data row2 col7\" >39</td>\n",
       "      <td id=\"T_5915a_row2_col8\" class=\"data row2 col8\" >348</td>\n",
       "      <td id=\"T_5915a_row2_col9\" class=\"data row2 col9\" >212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5915a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_5915a_row3_col0\" class=\"data row3 col0\" >ViT base patch-16 224</td>\n",
       "      <td id=\"T_5915a_row3_col1\" class=\"data row3 col1\" >ViT-DR</td>\n",
       "      <td id=\"T_5915a_row3_col2\" class=\"data row3 col2\" >base + diffusion augmentation</td>\n",
       "      <td id=\"T_5915a_row3_col3\" class=\"data row3 col3\" >0.000030</td>\n",
       "      <td id=\"T_5915a_row3_col4\" class=\"data row3 col4\" >0.621391</td>\n",
       "      <td id=\"T_5915a_row3_col5\" class=\"data row3 col5\" >0.700000</td>\n",
       "      <td id=\"T_5915a_row3_col6\" class=\"data row3 col6\" >515</td>\n",
       "      <td id=\"T_5915a_row3_col7\" class=\"data row3 col7\" >45</td>\n",
       "      <td id=\"T_5915a_row3_col8\" class=\"data row3 col8\" >291</td>\n",
       "      <td id=\"T_5915a_row3_col9\" class=\"data row3 col9\" >269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1e1bdec06a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_df = pd.DataFrame(eval_metrics)\n",
    "epoch_df[\"base_model_name\"] = \"ViT base patch-16 224\"\n",
    "epoch_df[\"finetuned_model_name\"] = \"ViT-DR\"\n",
    "epoch_df[\"finetune_data\"] = [\"base\" if \"base\" in model_name else \"base + diffusion augmentation\" for model_name in model_names]\n",
    "epoch_df[\"learning_rate\"] = [1e-05 if \"1e\" in model_name else 3e-05 for model_name in model_names]\n",
    "epoch_df[\"eval_accuracy\"] = epoch_df[\"eval_accuracy\"].apply(lambda x: x[\"accuracy\"])\n",
    "epoch_df[\"eval_confusion_matrix\"] = epoch_df[\"eval_confusion_matrix\"].apply(lambda x: x.flatten().tolist())\n",
    "epoch_df[[\"eval_tn\", \"eval_fp\", \"eval_fn\", \"eval_tp\"]] = pd.DataFrame(epoch_df[\"eval_confusion_matrix\"].tolist(), index=epoch_df.index)\n",
    "epoch_df = epoch_df[[\"base_model_name\", \"finetuned_model_name\", \"finetune_data\", \"learning_rate\", \"eval_loss\", \"eval_accuracy\",  \"eval_tn\", \"eval_fp\", \"eval_fn\", \"eval_tp\"]]\n",
    "epoch_df = epoch_df.style.apply(highlight_max, axis=None)\n",
    "display(epoch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
